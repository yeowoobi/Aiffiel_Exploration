{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caf34ad1",
   "metadata": {},
   "source": [
    "# Exploration 04\n",
    "## 작사가 인공지능 만들기\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f6caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import os                                                                                                                                                                                                      \n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4d7ada0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기:  187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME') + '/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "print(\"데이터 크기: \", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4023d0db",
   "metadata": {},
   "source": [
    "문장이 잘 불러와졌는지 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c6109f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I've heard there was a secret chord\n",
      "That David played, and it pleased the Lord\n",
      "But you don't really care for music, do you?\n",
      "It goes like this\n",
      "The fourth, the fifth\n",
      "The minor fall, the major lift\n",
      "The baffled king composing Hallelujah Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah Your faith was strong but you needed proof\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뛰기\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뛰기\n",
    "\n",
    "    if idx > 9: break  \n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2744c3cd",
   "metadata": {},
   "source": [
    "#### 텍스트 생성 모델을 만들기 위해서는 입력된 문장을 쪼개어 Tokenize 해야한다.\n",
    "아래와 같은 기준으로 문장을 tokenize 해보자.\n",
    "1. 소문자로 바꾸고, 양쪽 공백 지우기\n",
    "2. 특수문자 양쪽에 공백을 넣고, 여러개의 공백은 하나의 공백으로 바꾸기.\n",
    "3. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꾸고 다시 양쪽 공백을 지우기\n",
    "4. 문장 시작에는 `<start>`, 끝에는 `<end>`를 추가하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0447e570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test]\n",
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r'\\[[^)]*\\]', '', sentence)\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence\n",
    "print(\"[Test]\")\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f44c958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>',\n",
       " '<start> you saw her bathing on the roof <end>',\n",
       " '<start> her beauty and the moonlight overthrew her <end>',\n",
       " '<start> she tied you <end>',\n",
       " '<start> to a kitchen chair <end>',\n",
       " '<start> she broke your throne , and she cut your hair <end>',\n",
       " '<start> and from your lips she drew the hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah you say i took the name in vain <end>',\n",
       " '<start> i don t even know the name <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0:\n",
    "        continue\n",
    "    if preprocess_sentence(sentence) == '<start>  <end>':\n",
    "        continue\n",
    "        \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "corpus[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19970038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2633 ...    0    0    0]\n",
      " [   2   35    7 ...   44    3    0]\n",
      " ...\n",
      " [   5   22    9 ...   10 1009    3]\n",
      " [  37   15 9032 ...  873  641    3]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f4f51267ee0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words = 12000,\n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\")\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)\n",
    "    \n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff6bb82",
   "metadata": {},
   "source": [
    "문장의 단어를 15개로 제한하였으므로 `maxlen=15` 코드를 `tensor`에 추가하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aecd2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5   91  296   64   57    9  963 6030    3    0    0    0\n",
      "     0]\n",
      " [   2   17 2633  895    4    8   11 6031    6  329    3    0    0    0\n",
      "     0]\n",
      " [   2   35    7   37   15  164  281   28  299    4   47    7   44    3\n",
      "     0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(175406, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tensor[:3])\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acbb3ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba894fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    5   91  296   64   57    9  963 6030    3    0    0    0]\n",
      "[  50    5   91  296   64   57    9  963 6030    3    0    0    0    0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(175406, 14)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])\n",
    "src_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f0e7dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of source train set:  (140324, 14)\n",
      "shape of target train set:  (140324, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state = 32)\n",
    "\n",
    "print(\"shape of source train set: \", enc_train.shape)\n",
    "print(\"shape of target train set: \", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa6a595",
   "metadata": {},
   "source": [
    "총 데이터의 20% 를 평가 데이터 셋으로 사용하라고 하였으므로, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a23053e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset_train = dataset_train.shuffle(BUFFER_SIZE)\n",
    "dataset_train = dataset_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset_train\n",
    "\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
    "dataset_val = dataset_val.shuffle(BUFFER_SIZE)\n",
    "dataset_val = dataset_val.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46696fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 14\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f243b841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "548/548 [==============================] - 280s 477ms/step - loss: 3.7861 - val_loss: 3.3861\n",
      "Epoch 2/10\n",
      "548/548 [==============================] - 262s 477ms/step - loss: 3.2806 - val_loss: 3.2124\n",
      "Epoch 3/10\n",
      "548/548 [==============================] - 262s 478ms/step - loss: 3.0823 - val_loss: 3.0405\n",
      "Epoch 4/10\n",
      "548/548 [==============================] - 262s 478ms/step - loss: 2.8884 - val_loss: 2.9254\n",
      "Epoch 5/10\n",
      "548/548 [==============================] - 262s 477ms/step - loss: 2.7135 - val_loss: 2.8267\n",
      "Epoch 6/10\n",
      "548/548 [==============================] - 263s 479ms/step - loss: 2.5425 - val_loss: 2.7547\n",
      "Epoch 7/10\n",
      "548/548 [==============================] - 263s 479ms/step - loss: 2.3804 - val_loss: 2.6947\n",
      "Epoch 8/10\n",
      "548/548 [==============================] - 263s 479ms/step - loss: 2.2280 - val_loss: 2.6462\n",
      "Epoch 9/10\n",
      "548/548 [==============================] - 262s 478ms/step - loss: 2.0845 - val_loss: 2.6108\n",
      "Epoch 10/10\n",
      "548/548 [==============================] - 263s 479ms/step - loss: 1.9504 - val_loss: 2.5829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4eb0255940>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset_train, epochs=10, validation_data=dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927ee193",
   "metadata": {},
   "source": [
    "토큰의 개수가 15개가 되도록 하여 데이터의 shape이 (~,14)로 만들어졌다. 따라서 `embeding_size` 는 14로 설정하였다. <br>\n",
    "hidden_size의 경우 512 부터 계속 바꿔가며 학습을 시켜보았는데 `hidden_size`가 2048일 때가 더 낮은 `val_loss`를 보여주었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02abfe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "\n",
    "    while True:\n",
    "\n",
    "        predict = model(test_tensor) \n",
    " \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f57070de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c56d1abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> she is a perfect illusion <end> '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> she is\", max_len=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724667a6",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e1716c",
   "metadata": {},
   "source": [
    "## 평가\n",
    "노드에 나와있던 `i love` 로 시작하는 문장외에 `she go` 로 시작하는 문장으로 모델이 잘 학습되었는지를 확인한 결과, 각각 'i love you' 문장과 'she is a perfect illusion'의 문장이 출력된 것으로 보아 모델의 학습 자체는 잘 진행된 것으로 보인다. (두 번째 문장이 애매하긴 하다..)<br>\n",
    "다만 데이터의 개수도 목표한 개수만큼 줄이지 못했고 목표했던 `val_loss`에는 끝내 도달하지 못했다. <Br>\n",
    "`val_loss`가 목표치에 도달하지 못한 것이 적절한 하이퍼 파라미터값을 찾지 못한것, 그리고 데이터의 개수를 조금더 걸러내지 못했던 것이 원인이 아닐까 싶다.\n",
    "<br> 특히 `hidden_size`의 값을 올릴 경우 모델의 학습속도가 굉장히 느려지는 것을 확인할 수 있어서 적절한 값을 찾는 것이 정확도 뿐만 아니라 효율성에서도 매우 중요할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60f1c4a",
   "metadata": {},
   "source": [
    "## 후기\n",
    "최초로 진행한 NLP 프로젝트는 굉장히 낯설고 어렵게 느껴졌다. 이전에 했었던 CV 프로젝트에 비하여 직관적인 이해가 잘 되지 않았던 것 같다. 무엇보다 모델의 하이퍼 파라미터의 설정이 어려웠다. 노드 내용 외에 자료도 조금 더 찾아보았으나 이전에 공부했던 내용들에 비해 와닿지 않았던 것 같다. <br>\n",
    "여러모로 배워야할 것이 많다는 게 실감나는 프로젝트였다. 자연어 처리에 대한 전반적인 이해가 많이 부족하다고 느꼈으며 더 공부를 해 가면서 다시 이 프로젝트를 보았을 때 어떠한 이유에 의하여 이러한 결과가 나타났는지, 또 어떻게 하면 성능을 더 끌어낼 수 있는 지에 대해서 다시 확인할 수 있도록 하겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ceae77",
   "metadata": {},
   "source": [
    "## 3. Reference\n",
    "[1] https://algopoolja.tistory.com/34 \n",
    "_embedding size 내용 참고_ <br>\n",
    "[2] https://velog.io/@hwanython/%EC%9E%91%EC%82%AC%EA%B0%80-%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%A7%8C%EB%93%A4%EA%B8%B0 _코드에 대한 설명 참고_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
